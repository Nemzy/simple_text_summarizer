{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nemanja/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import data_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, Y, word2idx, idx2word, vocab, _ = data_utils.read_data_set('data.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First article headline - encoded:\n",
      "[3402, 6428, 48, 209, 1862]\n",
      "['ECB', 'defends', 'England', 'tour', 'schedule']\n",
      "\n",
      "First article text - encoded:\n",
      "[24, 48, 6, 146, 458, 732, 20, 1174, 364, 651, 5, 2, 23926, 7, 2, 209, 3, 135, 1988, 17146, 7, 2, 6540, 9212, 29, 287, 275, 5, 1771, 12159, 2, 51, 7, 76, 66, 3, 66, 3401, 3402, 554, 7, 269, 8285, 386, 3270, 101, 70, 2307, 147, 23107, 45, 12, 832, 2, 39, 11647, 26, 2, 135, 632, 11219, 10907, 135, 248, 14, 98, 36, 5795, 9795, 2101, 26, 51, 1483, 3, 2, 169, 7, 22748, 135, 248, 5244, 291, 2149, 3, 287, 1872, 6, 22, 2381, 35, 2696, 13, 183, 43, 594, 6, 393, 5854, 70, 250, 3, 62, 397, 148, 292, 6, 309, 295, 1524, 210, 2, 807, 7, 2, 136, 2101, 3, 4368, 2, 5203, 18, 3157, 7207, 57, 7, 451, 1612, 70, 32, 473, 3, 22829, 4, 1145, 1081, 7, 269, 50, 2, 1642, 2239, 10622, 182, 31, 4760, 18, 5659, 5, 4, 80, 5923, 142, 5, 1509, 7, 45, 2, 269, 9, 4752, 123, 63, 86, 2328, 18, 2, 9711, 7, 174, 269, 16, 2, 3766, 6, 135, 1974, 2101, 5, 17217, 1954, 16332, 658, 7, 147, 209, 30, 3, 19, 9795, 71, 4, 74, 5986, 4821, 70, 4587, 5, 4, 508, 3, 4220, 4, 24403, 5, 2, 176, 7, 148, 292, 95, 295, 10334, 2759, 17888, 61, 6592, 2, 1862, 10, 15100, 26, 20831, 8, 2, 68, 6, 38, 2, 2088, 7, 35, 5659, 10, 23566, 2, 4072, 3499, 413, 2, 1977, 70, 14, 3, 823, 147, 1588, 1917, 631, 13, 2, 135, 632, 7929, 6, 320, 18, 44, 1642, 25, 2027, 33, 4, 6724, 4772, 3270, 332, 517, 48, 322, 678, 10686, 4891, 1819, 140, 76, 513, 66, 3, 66, 23, 22316, 6, 38, 3270, 6, 48, 110, 2687, 2934, 161, 222, 14, 931, 3, 331, 46, 779, 2055, 16, 2416, 2, 7250, 1253, 7, 2, 48, 148, 1739, 118, 2, 200, 77, 10, 107, 392, 5, 7126, 81, 2, 3987, 22215, 148, 292, 5, 76, 6, 4, 247, 437, 9, 4, 402, 7534, 1007, 107, 645, 81, 2141, 12, 82, 538, 2, 39, 247, 7, 67, 376, 1049, 125, 11, 209, 6, 8, 109, 95, 199, 437, 6, 17, 1188, 117, 754, 3571, 140, 4, 449, 7, 1274, 870, 527, 9, 74, 12363, 505, 5214, 140, 199, 148, 292, 13, 107, 198, 5, 12985, 1394, 2, 142, 5136, 4540, 32, 1455, 195, 8201, 24, 198, 154, 7727, 6, 1429, 21, 1007, 74, 316, 84, 3, 469, 475, 1958, 195, 63, 4, 657, 7, 294, 3, 88, 11, 13, 331, 288, 57, 1019, 255, 2, 209, 10, 8892, 195, 58, 57, 29, 76, 148, 1390, 2423, 419, 4, 664, 5, 1771, 1889, 21, 1007, 107, 1512, 3, 19, 272, 128, 81, 331, 5765, 330, 1072, 5, 9542]\n",
      "\n",
      "Most freq. words:\n",
      "['the', 'to', 'a', 'in', 'and', 'of', 'for', 'is', 'was', 'on', 'he', 'with', 'have', 'his', 'at', 'I', 'that', 'be', 'has', 'but', 'will', 'as', 'The', 'it', 'from', 'not', 'by', 'after', 'had', 'we', 'are', 'been', 'who', 'their', 'an', '-', 'said', 'first', 'they', '\"I', 'out', 'against', 'this', 'when', 'But', 'would', 'England', 'were', 'over', 'last']\n"
     ]
    }
   ],
   "source": [
    "print 'First article headline - encoded:\\n', Y[0]\n",
    "print [idx2word[idx] for idx in Y[0]]\n",
    "print '\\nFirst article text - encoded:\\n', X[0]\n",
    "print '\\nMost freq. words:\\n', vocab[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 4\n",
    "word2idx['<pad>'] =  vocab_size - 2\n",
    "idx2word[vocab_size - 2] = '<pad>'\n",
    "word2idx['<go>'] =  vocab_size - 1\n",
    "idx2word[vocab_size - 1] = '<go>'\n",
    "\n",
    "# data padding\n",
    "def padding(x, y):\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        labels.append([word2idx['<go>']] + y[i] + [word2idx['<eos>']] + (8 - len(y[i])) * [word2idx['<pad>']])\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i]) % 2 + 1):\n",
    "            part = x[i][j*100:(j+1)*100]\n",
    "            part = (100 - len(part)) * [word2idx['<pad>']] + part\n",
    "            inputs.append((part, i)) \n",
    "    return inputs, labels\n",
    "\n",
    "# data spliting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y\n",
    "\n",
    "X_train, Y_train = padding(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bulding a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 100\n",
    "output_seq_len = 10\n",
    "\n",
    "# placeholders for sequences\n",
    "encoder_inputs = []\n",
    "for _ in range(input_seq_len):\n",
    "    encoder_inputs.append(tf.placeholder(tf.int32, shape = [None], name = 'encoder{}'.format(_)))\n",
    "\n",
    "decoder_inputs = []\n",
    "for _ in range(output_seq_len):\n",
    "    decoder_inputs.append(tf.placeholder(tf.int32, shape = [None], name = 'decoder{}'.format(_)))\n",
    "    \n",
    "targets = [decoder_inputs[i+1] for i in range(len(decoder_inputs)-1)]\n",
    "\n",
    "# output projection - dim reduction\n",
    "output_dim = 512\n",
    "w_t = tf.get_variable(\"proj_w\", [vocab_size, output_dim], dtype=tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "b = tf.get_variable(\"proj_b\", [vocab_size], dtype=tf.float32)\n",
    "output_projection = (w, b)\n",
    "\n",
    "model = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs, \n",
    "                                                decoder_inputs, \n",
    "                                                tf.nn.rnn_cell.BasicLSTMCell(output_dim),\n",
    "                                                num_encoder_symbols = vocab_size,\n",
    "                                                num_decoder_symbols = vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous= False,\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Definition of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sampled_loss(labels, logits):\n",
    "    \n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                weights=w_t,\n",
    "                biases=b,\n",
    "                labels=tf.reshape(labels, [-1, 1]),\n",
    "                inputs=logits,\n",
    "                num_sampled=128,\n",
    "                num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper function for feeding data into placeholders\n",
    "def feed_dict(x, y, batch_size = 200):\n",
    "    \n",
    "    idxes = np.random.choice([i for i in range(len(x))], size = batch_size)\n",
    "    \n",
    "    feed = {}\n",
    "    for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([x[j][0][i] for j in idxes])\n",
    "            \n",
    "    for i in range(output_seq_len):\n",
    "            feed[decoder_inputs[i].name] = np.array([y[x[j][1]][i] for j in idxes])\n",
    "            \n",
    "    return feed\n",
    "\n",
    "# predicting a headline for article\n",
    "def predict(sess, feed_dict):\n",
    "    \n",
    "    outputs, states = sess.run(model, feed_dict = feed_dict)\n",
    "    labels = sess.run(targets, feed_dict = feed_dict)\n",
    "    \n",
    "    return outputs, labels\n",
    "\n",
    "# output projection\n",
    "def proj(sess, output_seq):\n",
    "    \n",
    "    output_proj_ops = [tf.matmul(output_seq[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "    return np.array(sess.run(output_proj_ops))\n",
    "    \n",
    "# decoding predicted headline\n",
    "def decode_output_seq(output_seq):\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for t in range(output_seq_len):\n",
    "        smax = softmax(output_seq[t])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(idx2word[idx])\n",
    "        \n",
    "    return words\n",
    "\n",
    "# decoding a label\n",
    "def decode_label(label):\n",
    "    \n",
    "    words = []\n",
    "    for idx in label:\n",
    "        words.append(idx2word[idx])\n",
    "    return words\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted headline:\n",
      "personality. personality. primary primary primary primary primary primary primary primary \n",
      "\n",
      "Actual headline:\n",
      "England stutter to Zimbabwe win <eos> <pad> <pad> <pad> <pad> \n",
      "\n",
      "---------TRAINING---------\n",
      "\n",
      "\n",
      "step: 0, loss: 40.7962417603\n",
      "step: 9, loss: 35.7465782166\n",
      "Training time for 10 steps:40.6767930984s\n"
     ]
    }
   ],
   "source": [
    "steps = 10\n",
    "learning_rate = 1.0\n",
    "batch_size = 200\n",
    "\n",
    "labels_tensors = []\n",
    "for _ in range(output_seq_len):\n",
    "    labels_tensors.append(tf.placeholder(tf.int32, shape = [None], name = 'labels{}'.format(_)))\n",
    "    \n",
    "logits_tensors = []\n",
    "for _ in range(output_seq_len):\n",
    "    logits_tensors.append(tf.placeholder(tf.float32, shape = [None, output_dim], name = 'logits{}'.format(_)))\n",
    "    \n",
    "# calculate a loss for a whole seq\n",
    "def calculate_loss():\n",
    "    loss = sampled_loss(labels_tensors[0], logits_tensors[0])\n",
    "    \n",
    "    for i in range(1, output_seq_len):\n",
    "        loss += sampled_loss(labels_tensors[i], logits_tensors[i])\n",
    "        \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "loss = calculate_loss()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    feed = feed_dict(X_train, Y_train)\n",
    "    outputs, labels = predict(sess, feed)\n",
    "    # projecting only one output_seq not a whole batch - because of memory!   \n",
    "    output_seq = proj(sess, [outputs[i][199].reshape(1, output_dim) for i in range(output_seq_len)])\n",
    "    \n",
    "    # decoding predicted headline     \n",
    "    output_seq = np.reshape(output_seq, [output_seq_len, vocab_size])\n",
    "    words = decode_output_seq(output_seq)\n",
    "    print 'Predicted headline:'\n",
    "    for word in words:\n",
    "        print word,\n",
    "    print '\\n'\n",
    "    # decoding corresponding label     \n",
    "    label = [labels[i][199] for i in range(output_seq_len-1)] + [word2idx['<pad>']]\n",
    "    words = decode_label(label)\n",
    "    print 'Actual headline:'\n",
    "    for word in words:\n",
    "        print word,\n",
    "    print '\\n\\n---------TRAINING---------\\n\\n'\n",
    "    \n",
    "    # training\n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "        outputs, labels = predict(sess, feed)\n",
    "        labels.append(np.full(shape = [batch_size], fill_value = word2idx['<pad>']))\n",
    "        \n",
    "        feed = {}\n",
    "        for i in range(output_seq_len):\n",
    "            feed[labels_tensors[i].name] = labels[i]\n",
    "            feed[logits_tensors[i].name] = outputs[i]\n",
    "            \n",
    "        sess.run(optimizer, feed_dict = feed)\n",
    "        \n",
    "        if step % (steps-1) == 0 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print 'step: {}, loss: {}'.format(step, loss_value)\n",
    "            \n",
    "    print 'Training time for {} steps:{}s'.format(steps, time.time() - t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will train the model for more steps later and test it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
