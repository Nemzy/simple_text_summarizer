{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, Y, word2idx, idx2word, vocab = data_utils.read_data_set('data.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First article headline - encoded:\n",
      "[2636, 5032, 44, 195, 1249]\n",
      "['ECB', 'defends', 'England', 'tour', 'schedule']\n",
      "\n",
      "First article text - encoded:\n",
      "[19, 44, 6, 117, 481, 702, 21, 1279, 403, 636, 5, 2, 14744, 7, 2, 195, 3, 145, 218, 13107, 7, 2, 5366, 8742, 30, 307, 264, 5, 1805, 1745, 2, 55, 7, 79, 66, 3, 66, 435, 2636, 589, 7, 230, 5344, 410, 2369, 23, 60, 2424, 160, 9939, 48, 14, 906, 2, 41, 7204, 27, 2, 145, 709, 702, 4049, 145, 218, 15, 111, 37, 5800, 9278, 1393, 27, 55, 894, 3, 2, 171, 7, 1308, 145, 218, 4883, 258, 2251, 3, 307, 1779, 6, 24, 2081, 36, 1988, 13, 161, 46, 584, 6, 420, 991, 60, 267, 3, 59, 413, 135, 242, 6, 308, 312, 791, 240, 2, 669, 7, 2, 136, 1393, 3, 4268, 2, 4320, 17, 3095, 5047, 58, 7, 476, 230, 60, 33, 513, 3, 12894, 4, 1297, 1204, 7, 230, 47, 2, 1520, 357, 6289, 137, 32, 4570, 17, 5403, 5, 4, 85, 5394, 131, 5, 1410, 7, 48, 2, 230, 10, 119, 61, 69, 89, 1771, 17, 2, 7472, 7, 176, 230, 18, 2, 544, 6, 145, 2086, 1393, 5, 1178, 2019, 13717, 732, 7, 160, 195, 31, 3, 20, 9278, 84, 4, 76, 4674, 1018, 60, 4063, 5, 4, 483, 3, 4275, 4, 15755, 5, 2, 200, 7, 135, 242, 102, 312, 791, 1321, 2369, 50, 5561, 2, 1249, 12, 342, 27, 2510, 8, 2, 57, 6, 23, 2, 1953, 7, 36, 5403, 12, 11231, 2, 3834, 2681, 422, 2, 106, 60, 15, 3, 767, 160, 1486, 54, 701, 13, 2, 145, 709, 639, 6, 332, 17, 45, 1520, 25, 2217, 34, 4, 5934, 527, 2369, 115, 612, 44, 354, 670, 7268, 4366, 1939, 148, 79, 435, 66, 3, 66, 26, 7910, 6, 23, 2369, 6, 44, 104, 2340, 2722, 182, 241, 15, 955, 3, 25, 38, 815, 1210, 18, 1695, 2, 6105, 1398, 7, 2, 44, 135, 167, 130, 2, 225, 72, 12, 122, 322, 5, 4899, 87, 2, 1249, 3876, 135, 242, 5, 79, 6, 4, 216, 335, 10, 4, 411, 1249, 1149, 122, 654, 87, 17, 14, 23, 198, 2, 41, 216, 7, 73, 275, 829, 125, 11, 195, 6, 8, 105, 102, 204, 335, 6, 9, 1334, 109, 593, 2120, 148, 4, 516, 7, 1315, 156, 377, 10, 76, 704, 598, 4819, 148, 204, 135, 242, 13, 122, 156, 5, 283, 818, 2, 131, 4615, 3534, 33, 88, 162, 1165, 19, 156, 152, 6141, 6, 1478, 22, 1149, 76, 346, 63, 3, 539, 548, 262, 162, 69, 4, 610, 7, 314, 3, 95, 11, 13, 25, 60, 58, 1103, 280, 2, 195, 12, 2515, 162, 56, 58, 30, 79, 135, 242, 765, 464, 4, 734, 5, 1805, 1745, 22, 1149, 122, 1481, 3, 20, 271, 121, 87, 25, 2350, 310, 720, 5, 737]\n",
      "\n",
      "Most freq. words:\n",
      "['the', 'to', 'a', 'in', 'and', 'of', 'for', 'I', 'is', 'on', 'was', 'with', 'he', 'have', 'his', 'that', 'at', 'The', 'be', 'has', 'but', 'said', 'will', 'it', 'as', 'from', 'not', 'by', 'after', 'had', 'we', 'are', 'been', 'who', 'their', 'an', 'But', 'out', '-', 'first', 'they', 'game', 'England', 'this', 'against', 'over', 'when', 'would', 'He', 'win']\n"
     ]
    }
   ],
   "source": [
    "print 'First article headline - encoded:\\n', Y[0]\n",
    "print [idx2word[idx] for idx in Y[0]]\n",
    "print '\\nFirst article text - encoded:\\n', X[0]\n",
    "print '\\nMost freq. words:\\n', vocab[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 4\n",
    "word2idx['<pad>'] =  vocab_size - 2\n",
    "idx2word[vocab_size - 2] = '<pad>'\n",
    "word2idx['<go>'] =  vocab_size - 1\n",
    "idx2word[vocab_size - 1] = '<go>'\n",
    "\n",
    "# data padding\n",
    "def padding(x, y):\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        labels.append([word2idx['<go>']] + y[i] + [word2idx['<eos>']] + (8 - len(y[i])) * [word2idx['<pad>']])\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(0, len(x[i]) - 50, 50):\n",
    "            part = x[i][j:j+100]\n",
    "            part = (100 - len(part)) * [word2idx['<pad>']] + part\n",
    "            inputs.append((part, i)) \n",
    "    return inputs, labels\n",
    "\n",
    "# data spliting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y\n",
    "\n",
    "X_train, Y_train = padding(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bulding a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 100\n",
    "output_seq_len = 10\n",
    "\n",
    "# placeholders for sequences\n",
    "encoder_inputs = []\n",
    "for _ in range(input_seq_len):\n",
    "    encoder_inputs.append(tf.placeholder(tf.int32, shape = [None], name = 'encoder{}'.format(_)))\n",
    "\n",
    "decoder_inputs = []\n",
    "for _ in range(output_seq_len):\n",
    "    decoder_inputs.append(tf.placeholder(tf.int32, shape = [None], name = 'decoder{}'.format(_)))\n",
    "    \n",
    "targets = [decoder_inputs[i+1] for i in range(len(decoder_inputs)-1)]\n",
    "\n",
    "# output projection - dim reduction\n",
    "output_dim = 512\n",
    "w_t = tf.get_variable(\"proj_w\", [vocab_size, output_dim], dtype=tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "b = tf.get_variable(\"proj_b\", [vocab_size], dtype=tf.float32)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs, \n",
    "                                                decoder_inputs, \n",
    "                                                tf.nn.rnn_cell.BasicLSTMCell(output_dim),\n",
    "                                                num_encoder_symbols = vocab_size,\n",
    "                                                num_decoder_symbols = vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous= False,\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Definition of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sampled_loss(labels, logits):\n",
    "    \n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                weights=w_t,\n",
    "                biases=b,\n",
    "                labels=tf.reshape(labels, [-1, 1]),\n",
    "                inputs=logits,\n",
    "                num_sampled=256,\n",
    "                num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper function for feeding data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    \n",
    "    idxes = np.random.choice([i for i in range(len(x))], size = batch_size)\n",
    "    \n",
    "    feed = {}\n",
    "    for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([x[j][0][i] for j in idxes])\n",
    "            \n",
    "    for i in range(output_seq_len):\n",
    "            feed[decoder_inputs[i].name] = np.array([y[x[j][1]][i] for j in idxes])\n",
    "            \n",
    "    return feed\n",
    "    \n",
    "# decoding output seq - headline\n",
    "def decode_output_seq(output_seq):\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for t in range(output_seq_len):\n",
    "        smax = softmax(output_seq[t])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(idx2word[idx])\n",
    "        \n",
    "    return words\n",
    "\n",
    "# decoding label\n",
    "def decode_label(label):\n",
    "    \n",
    "    words = []\n",
    "    for idx in label:\n",
    "        words.append(idx2word[idx])\n",
    "    return words\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Definitions of params and ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "steps = 10\n",
    "learning_rate = 0.5\n",
    "batch_size = 64\n",
    "\n",
    "# adding one more target\n",
    "targets.append(np.full(shape = [batch_size], fill_value = word2idx['<pad>']))\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# calculate the loss for a whole seq\n",
    "def calculate_loss():\n",
    "    loss = sampled_loss(targets[0], outputs[0])\n",
    "    \n",
    "    for i in range(1, output_seq_len):\n",
    "        loss += sampled_loss(targets[i], outputs[i])\n",
    "        \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# loss and optimizer ops\n",
    "loss = calculate_loss()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# initialization op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# generating headlines for a batch of articles\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---THIS IS WHAT MODEL GENERATES BEFORE TRAINING---\n",
      "\n",
      "Predicted headline:\n",
      "continuity continuity continuity continuity continuity continuity continuity Bovina Bovina Bovina \n",
      "\n",
      "Actual headline:\n",
      "Trial date is set for Balco case <eos> <pad> <pad> \n",
      "\n",
      "---------TRAINING---------\n",
      "\n",
      "\n",
      "step: 0, loss: 55.201877594\n",
      "step: 9, loss: 38.7748413086\n",
      "Training time for 10 steps:15.2158019543s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    print '---THIS IS WHAT MODEL GENERATES BEFORE TRAINING---\\n'\n",
    "    feed = feed_dict(X_train, Y_train, 1)\n",
    "    output_sequences = forward_step(sess, feed)\n",
    "    \n",
    "    # decoding generated headline     \n",
    "    output_seq = np.reshape(output_sequences, [output_seq_len, vocab_size])\n",
    "    words = decode_output_seq(output_seq)\n",
    "    print 'Predicted headline:'\n",
    "    for word in words:\n",
    "        print word,\n",
    "    print '\\n'\n",
    "    \n",
    "    # decoding corresponding label\n",
    "    labels = sess.run(targets[:-1], feed_dict = feed)\n",
    "    label = [labels[i][0] for i in range(output_seq_len-1)] + [word2idx['<pad>']]\n",
    "    words = decode_label(label)\n",
    "    print 'Actual headline:'\n",
    "    for word in words:\n",
    "        print word,\n",
    "    print '\\n\\n---------TRAINING---------\\n\\n'\n",
    "    \n",
    "    # training\n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % (steps-1) == 0 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print 'step: {}, loss: {}'.format(step, loss_value)\n",
    "            \n",
    "    print 'Training time for {} steps:{}s'.format(steps, time.time() - t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## I will train the model for more steps later and test it!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
